from langchain_mistralai import ChatMistralAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from typing import List, Dict, Optional
import os
from dotenv import load_dotenv

load_dotenv()

class RAGSystem:
    """Syst√®me RAG combinant FAISS et Mistral"""
    
    def __init__(self, 
                 vector_store,
                 model_name: str = "mistral-large-latest",
                 temperature: float = 0.7):
        """
        Initialise le syst√®me RAG
        
        Args:
            vector_store: Instance de FAISSVectorStore
            model_name: Mod√®le Mistral √† utiliser
            temperature: Temp√©rature pour la g√©n√©ration
        """
        self.vector_store = vector_store
        
        api_key = os.getenv("MISTRAL_API_KEY")
        if not api_key:
            raise ValueError("MISTRAL_API_KEY non trouv√©e dans .env")
        
        # Initialiser le mod√®le Mistral
        self.llm = ChatMistralAI(
            model=model_name,
            temperature=temperature,
            api_key=api_key
        )
        
        # Template de prompt
        self.prompt_template = self._create_prompt_template()
        
        print(f"‚úì Syst√®me RAG initialis√©")
        print(f"  Mod√®le : {model_name}")
        print(f"  Temp√©rature : {temperature}")
    
    def _create_prompt_template(self) -> ChatPromptTemplate:
        """Cr√©e le template de prompt pour le RAG"""
        
        template = """Tu es un assistant intelligent sp√©cialis√© dans les recommandations d'√©v√©nements culturels.

Contexte : Voici les √©v√©nements pertinents trouv√©s dans la base de donn√©es :

{context}

Question de l'utilisateur : {question}

Instructions :
- Base ta r√©ponse UNIQUEMENT sur les √©v√©nements fournis dans le contexte
- Recommande les √©v√©nements les plus pertinents
- Inclus les informations pratiques : titre, lieu, date, description
- Si aucun √©v√©nement ne correspond, dis-le poliment
- Sois pr√©cis et utile
- Formate ta r√©ponse de mani√®re claire et structur√©e

R√©ponse :"""

        return ChatPromptTemplate.from_template(template)
    
    def _format_documents(self, results: List[tuple]) -> str:
        """
        Formate les documents r√©cup√©r√©s pour le contexte
        
        Args:
            results: Liste de tuples (document, score)
            
        Returns:
            Contexte format√©
        """
        context_parts = []
        
        for i, (doc, score) in enumerate(results, 1):
            metadata = doc['metadata']
            
            event_info = f"""
√âv√©nement {i} (pertinence: {score:.2f}):
- Titre: {metadata.get('event_title', 'N/A')}
- Lieu: {metadata.get('location_city', 'N/A')} - {metadata.get('location_name', 'N/A')}
- Date: {metadata.get('date_start', 'N/A')}
- Cat√©gorie: {metadata.get('category', 'N/A')}
- Description: {doc['text']}
- URL: {metadata.get('url', 'N/A')}
"""
            context_parts.append(event_info.strip())
        
        return "\n\n---\n".join(context_parts)
    
    def query(self, 
              question: str, 
              k: int = 5,
              min_score: float = 0.0) -> Dict:
        """
        Effectue une requ√™te RAG compl√®te
        
        Args:
            question: Question de l'utilisateur
            k: Nombre de documents √† r√©cup√©rer
            min_score: Score minimum de pertinence
            
        Returns:
            Dictionnaire contenant la r√©ponse et les m√©tadonn√©es
        """
        # 1. R√©cup√©ration (Retrieval)
        print(f"\nüîç Recherche pour : '{question}'")
        results = self.vector_store.search(question, k=k)
        
        # Filtrer par score
        results = [(doc, score) for doc, score in results if score >= min_score]
        
        if not results:
            return {
                'question': question,
                'answer': "Je n'ai trouv√© aucun √©v√©nement correspondant √† votre recherche.",
                'sources': [],
                'context': ""
            }
        
        print(f"‚úì {len(results)} √©v√©nements pertinents trouv√©s")
        
        # 2. Formatage du contexte
        context = self._format_documents(results)
        
        # 3. G√©n√©ration (Augmented Generation)
        print(f"ü§ñ G√©n√©ration de la r√©ponse...")
        
        # Construire le prompt
        messages = self.prompt_template.format_messages(
            context=context,
            question=question
        )
        
        # G√©n√©rer la r√©ponse
        response = self.llm.invoke(messages)
        answer = response.content
        
        print(f"‚úì R√©ponse g√©n√©r√©e ({len(answer)} caract√®res)")
        
        # 4. Pr√©parer le r√©sultat
        sources = [
            {
                'title': doc['metadata'].get('event_title', 'N/A'),
                'city': doc['metadata'].get('location_city', 'N/A'),
                'date': doc['metadata'].get('date_start', 'N/A'),
                'url': doc['metadata'].get('url', 'N/A'),
                'score': float(score)
            }
            for doc, score in results
        ]
        
        return {
            'question': question,
            'answer': answer,
            'sources': sources,
            'context': context,
            'num_sources': len(results)
        }
    
    def batch_query(self, questions: List[str], k: int = 5) -> List[Dict]:
        """
        Traite plusieurs questions
        
        Args:
            questions: Liste de questions
            k: Nombre de documents par requ√™te
            
        Returns:
            Liste de r√©sultats
        """
        results = []
        for question in questions:
            result = self.query(question, k=k)
            results.append(result)
        
        return results


if __name__ == "__main__":
    from src.vector.langchain_faiss import FAISSVectorStore
    
    print("="*70)
    print("TEST DU SYST√àME RAG")
    print("="*70)
    
    # Charger le vector store
    print("\nüìÇ Chargement de l'index FAISS...")
    vector_store = FAISSVectorStore(embedding_provider="huggingface")
    vector_store.load_index()
    
    # Initialiser le syst√®me RAG
    print("\nü§ñ Initialisation du syst√®me RAG...")
    rag = RAGSystem(vector_store)
    
    # Tests
    test_questions = [
        "Je cherche un concert de jazz √† Paris",
        "Quels sont les √©v√©nements gratuits pour enfants ?",
        "Y a-t-il des expositions d'art contemporain ?",
    ]
    
    for question in test_questions:
        print("\n" + "="*70)
        result = rag.query(question, k=3)
        
        print(f"\n‚ùì Question : {result['question']}")
        print(f"\nüí¨ R√©ponse :")
        print(result['answer'])
        
        print(f"\nüìö Sources ({result['num_sources']}) :")
        for i, source in enumerate(result['sources'], 1):
            print(f"  {i}. {source['title'][:60]} (score: {source['score']:.3f})")
    
    print("\n‚úÖ Tests termin√©s !")